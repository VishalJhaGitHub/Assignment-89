{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9c5689-db9c-4345-a7af-56ad33272d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective: The objective of this assignment is to assess students' understanding of batch normalization in artificial neural networks (ANN) and its impact on training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "081614b7-39fb-4d53-b7bb-660d2cde096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Theory and Concepts:\n",
    "\n",
    "#1. Explain the concept of batch normalization in the context of Artificial Neural Networks.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Batch normalization is a technique used to enhance the training of artificial neural networks (ANNs) by addressing the problem of internal covariate shift. Internal covariate shift refers to the change in the distribution of intermediate activations of a neural network layer as the parameters of the previous layers are updated during training. This can slow down the training process and make it more challenging for the network to converge to an optimal solution.\n",
    "\n",
    "#Batch normalization mitigates internal covariate shift by normalizing the inputs of each layer within a mini-batch of training examples. The process involves several steps:\n",
    "\n",
    "#1 - Calculation of Mean and Variance: For each mini-batch of training data, the mean and variance of the activations across the mini-batch are computed.\n",
    "\n",
    "#2 - Normalization: The activations of the current layer within the mini-batch are normalized by subtracting the mean and dividing by the square root of the variance. This step centers the distribution of activations around zero and scales it to have unit variance.\n",
    "\n",
    "#3 - Scale and Shift: After normalization, the normalized activations are scaled by a learnable parameter (gamma) and shifted by another learnable parameter (beta). These parameters allow the network to adapt the normalized activations to the specific requirements of the layer.\n",
    "\n",
    "#The mathematical formula for batch normalization can be expressed as follows, where x represents the input activations, mean and variance are computed over the mini-batch, and epsilon is a small constant to avoid division by zero:\n",
    "\n",
    "#y = gamma * (x - mean) / sqrt(variance + epsilon) + beta\n",
    "\n",
    "#The benefits of batch normalization are significant:\n",
    "\n",
    "#1 - Faster Convergence: By maintaining a more stable distribution of activations throughout the training process, batch normalization allows the network to converge more quickly.\n",
    "\n",
    "#2 - Higher Learning Rates: Batch normalization reduces the sensitivity of the network to the initial values of the weights and biases, enabling the use of higher learning rates without the risk of divergence.\n",
    "\n",
    "#3 - Regularization: The normalization process introduces some noise to the training process, acting as a form of regularization and reducing the need for other techniques like dropout.\n",
    "\n",
    "#4 - Improved Gradient Flow: Batch normalization helps stabilize gradient values, which facilitates training of deeper networks without vanishing or exploding gradient problems.\n",
    "\n",
    "#5 - Reduction in Internal Covariate Shift: The core purpose of batch normalization is to reduce the internal covariate shift, making the training process more stable and effective.\n",
    "\n",
    "\n",
    "#2. Describe the benefits of using batch normalization during training.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Using batch normalization during training provides several important benefits that contribute to more effective and efficient training of artificial neural networks. Here are the key benefits of using batch normalization:\n",
    "\n",
    "#1 - Faster Convergence: Batch normalization helps neural networks converge faster during training. By maintaining stable distributions of activations across layers, it reduces the need for extensive adjustments to network parameters and speeds up the learning process.\n",
    "\n",
    "#2 - Higher Learning Rates: With batch normalization, neural networks can often use higher learning rates without the risk of diverging during training. This faster learning allows the network to reach convergence more quickly and can lead to improved generalization performance.\n",
    "\n",
    "#3 - Regularization: Batch normalization acts as a form of regularization by adding noise to the activations within each layer. This noise helps prevent overfitting, leading to better generalization on unseen data. As a result, the need for dropout or other regularization techniques might be reduced.\n",
    "\n",
    "#4 - Reduced Dependency on Weight Initialization: Batch normalization reduces the sensitivity of neural networks to the choice of initial weights and biases. This reduces the need for careful manual initialization and makes it easier to train networks effectively.\n",
    "\n",
    "#5 - Stable Gradient Propagation: Neural networks with batch normalization tend to exhibit more stable gradients throughout the training process. This stability leads to faster convergence and reduces the likelihood of vanishing or exploding gradient problems, particularly in deeper networks.\n",
    "\n",
    "#6 - Support for Deeper Networks: Batch normalization enables the successful training of deeper networks. As networks become deeper, the internal covariate shift problem becomes more pronounced. Batch normalization mitigates this issue and allows for the training of deep architectures that might otherwise be difficult to optimize.\n",
    "\n",
    "#7 - Adaptive Learning: The learnable scaling and shifting parameters in batch normalization allow each layer to adapt its own distribution of activations. This adaptability contributes to the network's ability to learn more efficiently and effectively.\n",
    "\n",
    "#8 - Robustness to Hyperparameters: Batch normalization provides some robustness to hyperparameters, such as learning rate and weight initialization. This makes the training process less sensitive to hyperparameter choices, leading to more stable training outcomes.\n",
    "\n",
    "#9 - Consistency Across Batches: Batch normalization ensures that the distribution of activations remains consistent across different mini-batches during training. This consistency aids in generalization and helps prevent the network from learning to rely on specific batch statistics.\n",
    "\n",
    "\n",
    "#3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The working principle of batch normalization involves two key components: the normalization step and the learnable parameters. Let's delve into each of these components to understand how batch normalization operates within an artificial neural network:\n",
    "\n",
    "#1. Normalization Step:\n",
    "    \n",
    "#The normalization step in batch normalization aims to normalize the activations of a neural network layer within each mini-batch of training data. This involves centering the distribution of activations around zero and scaling it to have a unit variance. The steps for normalization are as follows:\n",
    "\n",
    "#1 - Mean and Variance Calculation: For each mini-batch of training examples, the mean (μ) and variance (σ²) of the activations within that batch are computed. These values represent the statistics of the mini-batch.\n",
    "\n",
    "#2 - Normalization: The activations within the mini-batch are then normalized using the computed mean and variance. The purpose of this normalization is to make the activations of each neuron within the layer more consistent across different inputs.\n",
    "\n",
    "#3 - Scaling and Shifting: After normalization, the normalized activations are scaled by a learnable parameter called \"gamma\" (γ) and shifted by another learnable parameter called \"beta\" (β). These parameters allow the network to adapt the normalized activations to the specific requirements of the layer. If the layer doesn't need scaling or shifting, the network can learn to set γ to 1 and β to 0.\n",
    "\n",
    "#The normalized and adjusted activations are then passed to the next layer in the network for further processing.\n",
    "\n",
    "#2. Learnable Parameters:\n",
    "    \n",
    "#The learnable parameters, γ (gamma) and β (beta), play a crucial role in batch normalization. These parameters are learned during the training process through backpropagation and gradient descent. Here's how they function:\n",
    "\n",
    "#1 - Gamma (γ): The gamma parameter scales the normalized activations. It allows the network to control the range of activations after normalization. If γ is close to 1, it retains the normalized values; if γ is greater than 1, it scales up the values; if γ is less than 1, it scales down the values.\n",
    "\n",
    "#2 - Beta (β): The beta parameter shifts the normalized activations. It provides the network with the ability to introduce a bias term to the normalized values. If β is set to 0, the shift is effectively neutral; otherwise, the shift introduces an offset to the normalized values.\n",
    "\n",
    "#3 - The introduction of γ and β parameters ensures that the neural network can still learn different scales and biases while benefiting from normalized activations. These parameters give the network flexibility to adapt the normalized activations to the specific requirements of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f1bd8a-58a1-446d-8660-1f0278cc8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (9000, 28, 28, 1)\n",
      "Shape of y_train: (9000, 10)\n",
      "Shape of x_val: (1000, 28, 28, 1)\n",
      "Shape of y_val: (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Q2. Implementation:\n",
    "\n",
    "#1. Choose a dataset of your choice (e.g., MNIST, CIFAR-10) and preprocess it.\n",
    "\n",
    "#Ans\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic dataset for demonstration\n",
    "num_samples = 10000\n",
    "num_classes = 10\n",
    "num_features = 784\n",
    "\n",
    "# Generate random data and labels\n",
    "x = np.random.random((num_samples, num_features))\n",
    "y = np.random.randint(num_classes, size=num_samples)\n",
    "\n",
    "# Reshape and normalize the data\n",
    "x = x.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_onehot = np.zeros((num_samples, num_classes))\n",
    "y_onehot[np.arange(num_samples), y] = 1\n",
    "\n",
    "# Split into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y_onehot, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print shapes of the preprocessed data\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of x_val:\", x_val.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c115ffd-3068-48dc-b357-30502bd97db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:07<00:00, 1350548.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 2656484.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1186091.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1092597.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Epoch [1/10], Loss: 1.5760\n",
      "Epoch [2/10], Loss: 1.5280\n",
      "Epoch [3/10], Loss: 1.5034\n",
      "Epoch [4/10], Loss: 1.4909\n",
      "Epoch [5/10], Loss: 1.5260\n",
      "Epoch [6/10], Loss: 1.4714\n",
      "Epoch [7/10], Loss: 1.5083\n",
      "Epoch [8/10], Loss: 1.5242\n",
      "Epoch [9/10], Loss: 1.4769\n",
      "Epoch [10/10], Loss: 1.4671\n",
      "Accuracy on test set: 0.9697\n"
     ]
    }
   ],
   "source": [
    "#Q2. 2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,TensorFlow, PyTorch).\n",
    "\n",
    "#Ans\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a simple feedforward neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "dataloader = torch.utils.data.DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 28 * 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c163a62-1759-44da-8f35-d74e64b23a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.5774\n",
      "Epoch [2/10], Loss: 1.5041\n",
      "Epoch [3/10], Loss: 1.5188\n",
      "Epoch [4/10], Loss: 1.4652\n",
      "Epoch [5/10], Loss: 1.5524\n",
      "Epoch [6/10], Loss: 1.5084\n",
      "Epoch [7/10], Loss: 1.5643\n",
      "Epoch [8/10], Loss: 1.4925\n",
      "Epoch [9/10], Loss: 1.4848\n",
      "Epoch [10/10], Loss: 1.5407\n",
      "Accuracy on test set: 0.9645\n"
     ]
    }
   ],
   "source": [
    "#Q2. 3. Train the neural network on the chosen dataset without using batch normalization.\n",
    "\n",
    "#Ans\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a simple feedforward neural network class without batch normalization\n",
    "class SimpleNNWithoutBN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNNWithoutBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "dataloader = torch.utils.data.DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 28 * 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "model = SimpleNNWithoutBN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b344005f-32d5-4a4b-ab6c-d0cab75537e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4967\n",
      "Epoch [2/10], Loss: 1.6150\n",
      "Epoch [3/10], Loss: 1.5362\n",
      "Epoch [4/10], Loss: 1.5674\n",
      "Epoch [5/10], Loss: 1.5026\n",
      "Epoch [6/10], Loss: 1.4891\n",
      "Epoch [7/10], Loss: 1.4720\n",
      "Epoch [8/10], Loss: 1.4968\n",
      "Epoch [9/10], Loss: 1.4777\n",
      "Epoch [10/10], Loss: 1.5000\n",
      "Accuracy on test set: 0.9889\n"
     ]
    }
   ],
   "source": [
    "#Q2. 4. Implement batch normalization layers in the neural network and train the model again.\n",
    "\n",
    "#Ans\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a simple feedforward neural network class with batch normalization\n",
    "class SimpleNNWithBN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNNWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "dataloader = torch.utils.data.DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = 28 * 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "model = SimpleNNWithBN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0144f4e2-d112-4414-b3eb-7afe20d6f163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0796, Val Loss: 0.0796, Val Acc: 0.9468\n",
      "Epoch [2/10], Loss: 0.1445, Val Loss: 0.1445, Val Acc: 0.9608\n",
      "Epoch [3/10], Loss: 0.0094, Val Loss: 0.0094, Val Acc: 0.9692\n",
      "Epoch [4/10], Loss: 0.0341, Val Loss: 0.0341, Val Acc: 0.9727\n",
      "Epoch [5/10], Loss: 0.1912, Val Loss: 0.1912, Val Acc: 0.9704\n",
      "Epoch [6/10], Loss: 0.1700, Val Loss: 0.1700, Val Acc: 0.9753\n",
      "Epoch [7/10], Loss: 0.0226, Val Loss: 0.0226, Val Acc: 0.9764\n",
      "Epoch [8/10], Loss: 0.0359, Val Loss: 0.0359, Val Acc: 0.9791\n",
      "Epoch [9/10], Loss: 0.0049, Val Loss: 0.0049, Val Acc: 0.9760\n",
      "Epoch [10/10], Loss: 0.0046, Val Loss: 0.0046, Val Acc: 0.9796\n",
      "Epoch [1/10], Loss: 0.2206, Val Loss: 0.2206, Val Acc: 0.9718\n",
      "Epoch [2/10], Loss: 0.0529, Val Loss: 0.0529, Val Acc: 0.9773\n",
      "Epoch [3/10], Loss: 0.1803, Val Loss: 0.1803, Val Acc: 0.9805\n",
      "Epoch [4/10], Loss: 0.0985, Val Loss: 0.0985, Val Acc: 0.9792\n",
      "Epoch [5/10], Loss: 0.0055, Val Loss: 0.0055, Val Acc: 0.9830\n",
      "Epoch [6/10], Loss: 0.0512, Val Loss: 0.0512, Val Acc: 0.9823\n",
      "Epoch [7/10], Loss: 0.0051, Val Loss: 0.0051, Val Acc: 0.9829\n",
      "Epoch [8/10], Loss: 0.0193, Val Loss: 0.0193, Val Acc: 0.9822\n",
      "Epoch [9/10], Loss: 0.0093, Val Loss: 0.0093, Val Acc: 0.9849\n",
      "Epoch [10/10], Loss: 0.0008, Val Loss: 0.0008, Val Acc: 0.9829\n",
      "Model without Batch Normalization:\n",
      "Train Accuracy: 0.9796, Validation Accuracy: 0.9796\n",
      "Train Loss: 0.0046, Validation Loss: 0.0046\n",
      "Model with Batch Normalization:\n",
      "Train Accuracy: 0.9829, Validation Accuracy: 0.9829\n",
      "Train Loss: 0.0008, Validation Loss: 0.0008\n"
     ]
    }
   ],
   "source": [
    "#Q2. 5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.\n",
    "\n",
    "#Ans\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a simple feedforward neural network without batch normalization\n",
    "class NetWithoutBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWithoutBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define a simple feedforward neural network with batch normalization\n",
    "class NetWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)  # Batch normalization layer\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)  # Batch normalization layer\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# Loaders for training and validation data\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize models and optimizers\n",
    "model_without_bn = NetWithoutBN()\n",
    "model_with_bn = NetWithBN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_without_bn = optim.SGD(model_without_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer_with_bn = optim.SGD(model_with_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "train_losses_without_bn = []\n",
    "val_losses_without_bn = []\n",
    "train_accuracies_without_bn = []\n",
    "val_accuracies_without_bn = []\n",
    "\n",
    "train_losses_with_bn = []\n",
    "val_losses_with_bn = []\n",
    "train_accuracies_with_bn = []\n",
    "val_accuracies_with_bn = []\n",
    "\n",
    "# Training loop for model without batch normalization\n",
    "for epoch in range(num_epochs):\n",
    "    model_without_bn.train()\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        optimizer_without_bn.zero_grad()\n",
    "        outputs = model_without_bn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_without_bn.step()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model_without_bn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.view(-1, 28 * 28)\n",
    "            outputs = model_without_bn(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    val_loss = loss.item()\n",
    "    \n",
    "    train_losses_without_bn.append(loss.item())\n",
    "    val_losses_without_bn.append(val_loss)\n",
    "    train_accuracies_without_bn.append(correct / total)\n",
    "    val_accuracies_without_bn.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "# Training loop for model with batch normalization\n",
    "for epoch in range(num_epochs):\n",
    "    model_with_bn.train()\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        optimizer_with_bn.zero_grad()\n",
    "        outputs = model_with_bn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_with_bn.step()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model_with_bn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.view(-1, 28 * 28)\n",
    "            outputs = model_with_bn(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    val_loss = loss.item()\n",
    "    \n",
    "    train_losses_with_bn.append(loss.item())\n",
    "    val_losses_with_bn.append(val_loss)\n",
    "    train_accuracies_with_bn.append(correct / total)\n",
    "    val_accuracies_with_bn.append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "# Print and compare metrics\n",
    "print(\"Model without Batch Normalization:\")\n",
    "print(f\"Train Accuracy: {train_accuracies_without_bn[-1]:.4f}, Validation Accuracy: {val_accuracies_without_bn[-1]:.4f}\")\n",
    "print(f\"Train Loss: {train_losses_without_bn[-1]:.4f}, Validation Loss: {val_losses_without_bn[-1]:.4f}\")\n",
    "\n",
    "print(\"Model with Batch Normalization:\")\n",
    "print(f\"Train Accuracy: {train_accuracies_with_bn[-1]:.4f}, Validation Accuracy: {val_accuracies_with_bn[-1]:.4f}\")\n",
    "print(f\"Train Loss: {train_losses_with_bn[-1]:.4f}, Validation Loss: {val_losses_with_bn[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c0f092f-38fd-45a3-8a20-0c7d09797bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. 6. Discuss the impact of batch normalization on the training process and the performance of the neural network.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Batch normalization has a significant impact on the training process and the performance of neural networks. Here's a discussion of its effects:\n",
    "\n",
    "#1. Accelerated Training:\n",
    "#Batch normalization helps accelerate the training process by reducing the internal covariate shift. Internal covariate shift refers to the change in the distribution of intermediate layer activations during training. By normalizing the inputs of each layer, batch normalization helps in maintaining a stable distribution of activations, which allows for faster convergence. As a result, the network requires fewer epochs to achieve good performance.\n",
    "\n",
    "#2. Increased Learning Rates:\n",
    "#Batch normalization enables the use of higher learning rates during training. This is due to the normalization step, which reduces the sensitivity of network parameters to the scale of input data. Higher learning rates can speed up convergence and help the network escape local minima.\n",
    "\n",
    "#3. Regularization Effect:\n",
    "#Batch normalization introduces a slight regularization effect, reducing the need for other regularization techniques such as dropout or L2 regularization. It helps mitigate overfitting by adding noise to the activations, making the network more robust and preventing extreme activations.\n",
    "\n",
    "#4. Gradient Flow and Vanishing/Exploding Gradients:\n",
    "#Batch normalization helps in addressing the vanishing and exploding gradient problems. By maintaining activations close to zero mean and unit variance, it stabilizes the gradients flowing through the network. This enables more stable and efficient backpropagation, especially in deep networks.\n",
    "\n",
    "#5. Reducing Internal Covariate Shift:\n",
    "#Internal covariate shift can slow down training because each layer's parameters need to adapt to the changing distribution of previous layer's activations. Batch normalization reduces this shift by keeping activations normalized, which leads to more stable weight updates and faster convergence.\n",
    "\n",
    "#6. Robustness to Initialization:\n",
    "#Batch normalization reduces the dependence of network training on the choice of initialization values for weights. It allows networks to converge successfully even when weights are initialized randomly, making the training process more consistent and less sensitive to initialization choices.\n",
    "\n",
    "#7. Handling Different Batch Sizes:\n",
    "#Batch normalization makes neural networks more robust to different batch sizes during training. While other normalization techniques might require careful re-tuning of hyperparameters when batch sizes change, batch normalization adapts effectively to different batch sizes, making it more versatile.\n",
    "\n",
    "#8. Higher Performance:\n",
    "#In terms of performance, batch normalization often results in higher accuracy and lower validation loss. It improves the generalization of the model by reducing overfitting and allowing the model to learn more discriminative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252686da-bbcd-4178-85f4-a873ded863e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. Experimentation and Analysis:\n",
    "\n",
    "#1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Experimenting with different batch sizes can indeed have an impact on the training dynamics and model performance. Here's how different batch sizes can affect your neural network training:\n",
    "\n",
    "#Effect on Training Dynamics:\n",
    "\n",
    "#1 - Larger Batch Sizes:\n",
    "\n",
    "#Faster Training: Larger batch sizes can speed up training since more data is processed in parallel, utilizing hardware resources efficiently.\n",
    "#Smoother Loss Curve: With larger batch sizes, the loss curve tends to be smoother, which can help prevent the model from getting stuck in local minima.\n",
    "#Reduced Noise: Larger batch sizes can reduce the stochasticity in gradient updates, which may lead to more stable training.\n",
    "\n",
    "#2 - Smaller Batch Sizes:\n",
    "\n",
    "#Noisier Loss Curve: Smaller batch sizes introduce more randomness in gradient estimates, leading to a noisier loss curve. This can be beneficial in escaping local minima.\n",
    "#Slower Convergence: Smaller batch sizes require more iterations to cover the entire training dataset, potentially leading to slower convergence.\n",
    "#Regularization Effect: Smaller batch sizes introduce a regularization effect similar to dropout, as the network is exposed to slightly different data samples in each iteration.\n",
    "\n",
    "#Effect on Model Performance:\n",
    "\n",
    "#1 - Larger Batch Sizes:\n",
    "\n",
    "#Generalization: Larger batch sizes might lead to slightly worse generalization on the validation set. This is because the model updates are more deterministic, potentially causing overfitting.\n",
    "#Resource Intensive: Larger batch sizes require more memory and computational resources. They might not fit into the memory of GPUs with limited VRAM.\n",
    "\n",
    "#2 - Smaller Batch Sizes:\n",
    "\n",
    "#Better Generalization: Smaller batch sizes tend to generalize better, as they introduce more noise and variations in the training process, which can prevent overfitting.\n",
    "#Computational Efficiency: Smaller batch sizes might not fully utilize the hardware resources, as GPUs are often more efficient with larger batches.\n",
    "\n",
    "#Tips for Experimentation:\n",
    "\n",
    "#1 - Start with Default: Begin with a moderate batch size that's commonly used, like 32 or 64. These sizes are often well-suited for many datasets and architectures.\n",
    "\n",
    "#2 - Grid Search: If you have the resources, perform a grid search with a range of batch sizes (e.g., 8, 16, 32, 64, 128) and compare their effects on training and validation performance.\n",
    "\n",
    "#3 - Monitor Learning Dynamics: Pay attention to the loss curves during training. Smoother curves are desirable, but be cautious about sharp decreases that might indicate convergence to a local minimum.\n",
    "\n",
    "#4 - Regularization vs. Speed: Smaller batch sizes introduce some regularization, which can be beneficial for improving generalization. However, they might lead to slower convergence due to frequent updates.\n",
    "\n",
    "#5 - Balance with Resources: Consider your available hardware resources. Larger batch sizes might be more efficient on powerful GPUs, while smaller batch sizes might be suitable for limited VRAM.\n",
    "\n",
    "#6 - Fine-Tuning: After identifying a promising batch size range, consider fine-tuning other hyperparameters (e.g., learning rate, dropout rate) to optimize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8ee93f-8ac9-4218-8540-327cbe9d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. 2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks.\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Advantages of Batch Normalization:\n",
    "\n",
    "#1 - Improved Convergence Speed: Batch normalization accelerates the convergence of training by reducing the internal covariate shift. This means that the network requires fewer epochs to reach a similar level of performance.\n",
    "\n",
    "#2 - Stable Gradient Flow: Batch normalization normalizes the input to each layer, which helps in maintaining a stable gradient flow during backpropagation. This stability can prevent issues like vanishing and exploding gradients.\n",
    "\n",
    "#3 - Higher Learning Rates: Batch normalization allows for the use of higher learning rates. The normalization reduces the sensitivity of the network's weights to the initial values, enabling faster learning without diverging.\n",
    "\n",
    "#4 - Regularization Effect: Batch normalization introduces a slight amount of noise to the network due to the randomness in mini-batch statistics. This noise acts as a form of regularization, reducing overfitting.\n",
    "\n",
    "#5 - Reduced Need for Careful Initialization: With batch normalization, the network is less sensitive to the choice of initial weights. This can simplify the process of setting up and training neural networks.\n",
    "\n",
    "#6 - Generalization Improvement: Batch normalization's regularization effect can lead to better generalization, as it helps in reducing overfitting on the training data.\n",
    "\n",
    "#Potential Limitations of Batch Normalization:\n",
    "\n",
    "#1 - Batch Size Dependency: Batch normalization's effectiveness is somewhat dependent on the batch size used during training. Extremely small batch sizes might result in noisy batch statistics, affecting the normalization process.\n",
    "\n",
    "#2 - Test-Time Behavior: During test time (inference), the network might not always receive mini-batches. Different normalization techniques, like using the population statistics, are required during inference, which can introduce a slight mismatch between training and inference.\n",
    "\n",
    "#3 - Training Time Overhead: Batch normalization adds a computational overhead during training as it requires calculating batch statistics and normalizing the data in each forward pass.\n",
    "\n",
    "#4 - Hyperparameter Sensitivity: Batch normalization introduces additional hyperparameters like the learning rate of the moving averages and the scaling and shifting parameters. Tuning these hyperparameters might be required for optimal performance.\n",
    "\n",
    "#5 - Dependency on Network Architecture: While batch normalization works well for many architectures, its effectiveness can vary for certain types of networks or tasks.\n",
    "\n",
    "#6 - GPU Memory Consumption: In some cases, especially with larger batch sizes, the intermediate normalized activations can consume significant GPU memory, limiting the model's scalability.\n",
    "\n",
    "#7 - Not Always Necessary: For small and shallow networks or simple tasks, the benefits of batch normalization might not outweigh the computational and memory overhead it introduces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be6080-d956-4029-b3f1-b2351603c57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
